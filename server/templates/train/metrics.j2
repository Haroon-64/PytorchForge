{% for metric in config.training.metrics %}
metrics = []
{% if metric.name == 'Accuracy' %}
metrics.append(torchmetrics.Accuracy(
    task='{{ metric.params.task | default("multiclass") }}',
    num_classes={{ metric.params.num_classes | default(2) }},
    threshold={{ metric.params.threshold | default('None') }},
    top_k={{ metric.params.top_k | default('None') }},
    average='{{ metric.params.average | default("micro") }}'
))
{% elif metric.name == 'F1Score' %}
metrics.append(torchmetrics.F1Score(
    task='{{ metric.params.task | default("multiclass") }}',
    num_classes={{ metric.params.num_classes | default(2) }},
    threshold={{ metric.params.threshold | default('None') }},
    top_k={{ metric.params.top_k | default('None') }},
    average='{{ metric.params.average | default("micro") }}'
))
{% elif metric.name == 'Recall' %}
metrics.append(torchmetrics.Recall(
    task='{{ metric.params.task | default("multiclass") }}',
    num_classes={{ metric.params.num_classes | default(2) }},
    threshold={{ metric.params.threshold | default('None') }},
    top_k={{ metric.params.top_k | default('None') }},
    average='{{ metric.params.average | default("micro") }}'
))
{% elif metric.name == 'MeanAbsoluteError' %}
metrics.append(torchmetrics.MeanAbsoluteError(
    num_outputs={{ metric.params.num_outputs | default('None') }}
))
{% endif %}
{% endfor %}
