{% set opt = config.training.optimizer %}
{% if opt.name == 'Adam' %}
optimizer = torch.optim.Adam(
  model.parameters(),
  lr={{ opt.params.lr | default(0.001) }},
  betas={{ opt.params.betas | default([0.9, 0.999]) }},
  eps={{ opt.params.eps | default(1e-8) }},
  weight_decay={{ opt.params.weight_decay | default(0) }},
  amsgrad={{ 'True' if opt.params.amsgrad else 'False' if 'amsgrad' in opt.params else 'False' }}
)
{% elif opt.name == 'SGD' %}
optimizer = torch.optim.SGD(
  model.parameters(),
  lr={{ opt.params.lr | default(0.001) }},
  momentum={{ opt.params.momentum | default(0) }},
  weight_decay={{ opt.params.weight_decay | default(0) }},
  dampening={{ opt.params.dampening | default(0) }},
  nesterov={{ 'True' if opt.params.nesterov else 'False' if 'nesterov' in opt.params else 'False' }}
)
{% elif opt.name == 'RMSprop' %}
optimizer = torch.optim.RMSprop(
  model.parameters(),
  lr={{ opt.params.lr | default(0.01) }},
  alpha={{ opt.params.alpha | default(0.99) }},
  eps={{ opt.params.eps | default(1e-8) }},
  weight_decay={{ opt.params.weight_decay | default(0) }},
  momentum={{ opt.params.momentum | default(0) }},
  centered={{ 'True' if opt.params.centered else 'False' if 'centered' in opt.params else 'False' }}
)
{% elif opt.name == 'Adagrad' %}
optimizer = torch.optim.Adagrad(
  model.parameters(),
  lr={{ opt.params.lr | default(0.01) }},
  lr_decay={{ opt.params.lr_decay | default(0) }},
  weight_decay={{ opt.params.weight_decay | default(0) }},
  initial_accmulator_value={{ opt.params.initial_accumulator | default(0) }}
)
{% elif opt.name == 'NAdam' %}
optimizer = torch.optim.NAdam(
  model.parameters(),
  lr={{ opt.params.lr | default(0.002) }},
  betas={{ opt.params.betas | default([0.9, 0.999]) }},
  eps={{ opt.params.eps | default(1e-8) }},
  weight_decay={{ opt.params.weight_decay | default(0) }}
)
{% else %}
raise ValueError("Unsupported optimizer: {{ opt.name }}")
{% endif %}
