

class TextSummarizationDataset(Dataset):
    def __init__(self, root, split='{{ split | default("train") }}', tokenizer_name='{{ tokenizer_name | default("bert-base-uncased") }}', return_format='{{ return_format | default("dict") }}', max_length={{ max_length | default(512) }}, padding='{{ padding | default("max_length") }}', add_special_tokens={{ add_special_tokens | default("True") }}, return_attn_masks={{ return_attn_masks | default("True") }}, truncation='{{ truncation | default("right") }}', use_fast={{ use_fast | default("True") }}):
        self.root = Path(root) / split
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=use_fast)
        self.max_length = max_length
        self.padding = padding
        self.return_format = return_format
        self.return_attn_masks = return_attn_masks
        self.truncation = truncation == 'right'
        self.data = self._load_data()

    def _load_data(self):
        srcs = sorted((self.root / 'document').glob('*.txt'))
        return [(doc.read_text().strip(), Path(str(doc).replace('document', 'summary')).read_text().strip()) for doc in srcs]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        document, summary = self.data[idx]
        enc = self.tokenizer(document, max_length=self.max_length, padding=self.padding, truncation=self.truncation, return_attention_mask=self.return_attn_masks, add_special_tokens=add_special_tokens, return_tensors='pt')
        input_ids = enc['input_ids'].squeeze(0)
        attention_mask = enc['attention_mask'].squeeze(0) if self.return_attn_masks else None

        {% if self.return_format == 'tuple' %}
        return input_ids, summary
        {% elif return_format == 'raw' %}
        return input_ids
        {% else %}
        out = {'input_ids': input_ids, 'summary': summary}
        {% if return_attn_masks %}
        out['attention_mask'] = attention_mask
        {% endif %}
        return out
        {% endif %}

def get_loader(root, batch_size=8, split='train', shuffle=True, **kwargs):
    dataset = TextSummarizationDataset(root=root, split=split, **kwargs)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
